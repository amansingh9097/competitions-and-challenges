{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7a8e012",
   "metadata": {},
   "source": [
    "#### Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90b8abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# Load reference doc\n",
    "base_path = os.path.dirname(__file__)\n",
    "ref_path = os.path.join(base_path, \"explanation_guide.txt\")\n",
    "\n",
    "\n",
    "def load_docs():\n",
    "    with open(ref_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw = f.read()\n",
    "        # split by blank lines\n",
    "        paragraphs = [p.strip() for p in raw.split(\"\\n\\n\") if p.strip()]\n",
    "    return paragraphs\n",
    "\n",
    "\n",
    "docs = load_docs()\n",
    "vectorizer = TfidfVectorizer().fit(docs)\n",
    "doc_vectors = vectorizer.transform(docs)\n",
    "\n",
    "\n",
    "def retrieve_context(query, top_k=3):\n",
    "    \"\"\"Retrieve top K relevant chunks from the guide.\"\"\"\n",
    "    q_vec = vectorizer.transform([query])\n",
    "    scores = cosine_similarity(q_vec, doc_vectors)[0]\n",
    "    top_idx = scores.argsort()[-top_k:][::-1]\n",
    "    return [docs[i] for i in top_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095c26ad",
   "metadata": {},
   "source": [
    "#### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a95b244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "# Dummy LLM call (replace with OpenAI API)\n",
    "def call_llm(prompt: str) -> str:\n",
    "    return \"[LLM Explanation Placeholder]\\n\" + prompt[:200] + \"...\"\n",
    "\n",
    "\n",
    "# Optional helper to format key features\n",
    "def top_features_string(features: dict, top_k=5):\n",
    "    items = list(features.items())[:top_k]\n",
    "    return \"\\n\".join([f\"- {k}: {v}\" for k, v in items])\n",
    "\n",
    "\n",
    "def interpret_prediction(prediction, score):\n",
    "    if prediction == 1:\n",
    "        return \"High Risk\"\n",
    "    return \"Low Risk\"\n",
    "\n",
    "\n",
    "def explain_prediction(features: dict, prediction: int, risk_score: float):\n",
    "    label = interpret_prediction(prediction, risk_score)\n",
    "\n",
    "    query = f\"{label}, {', '.join(list(features.keys())[:3])}\"\n",
    "\n",
    "    retrieved = retrieve_context(query)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI banking risk analyst. Explain the model's decision.\n",
    "    \n",
    "    Model Prediction: {label} (risk score = {risk_score:.2f})\n",
    "\n",
    "    Customer Features:\n",
    "    {top_features_string(features)}\n",
    "\n",
    "    Relevant Context:\n",
    "    {json.dumps(retrieved, indent=2)}\n",
    "\n",
    "    Write a 4-6 line explanation in simple, factual terms.\n",
    "    \"\"\"\n",
    "    response = call_llm(prompt)\n",
    "\n",
    "    return {\n",
    "        \"prediction\": label,\n",
    "        \"risk_score\": risk_score,\n",
    "        \"retrieved_context\": retrieved,\n",
    "        \"explanation\": response,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aacef6",
   "metadata": {},
   "source": [
    "#### Example Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e228416d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example real row (replace with real dataset row)\n",
    "features = {\n",
    "\"income\": 4800,\n",
    "\"utilization\": 0.82,\n",
    "\"missed_payments\": 3,\n",
    "\"credit_history_length\": 1.2,\n",
    "\"age\": 24,\n",
    "\"debt_to_income\": 0.54,\n",
    "}\n",
    "\n",
    "\n",
    "prediction = 1\n",
    "risk_score = 0.92\n",
    "\n",
    "\n",
    "result = explain_prediction(features, prediction, risk_score)\n",
    "\n",
    "\n",
    "print(\"=== MODEL OUTPUT ===\")\n",
    "print(result[\"prediction\"], result[\"risk_score\"])\n",
    "\n",
    "\n",
    "print(\"\\n=== RETRIEVED CONTEXT ===\")\n",
    "for c in result[\"retrieved_context\"]:\n",
    "print(\"-\", c)\n",
    "\n",
    "\n",
    "print(\"\\n=== FINAL EXPLANATION ===\")\n",
    "print(result[\"explanation\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adcb-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
