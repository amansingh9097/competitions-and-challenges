{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dd30e74",
   "metadata": {},
   "source": [
    "# Classifier for Credit Default Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "efe51fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import roc_curve, auc, recall_score, precision_score, accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib\n",
    "import dill as pickle\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import SelectFromModel, RFE\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "79391a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30001, 25)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel(\"../data/raw/default of credit card clients.xls\", sheet_name=\"Data\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7c920766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>...</th>\n",
       "      <th>X15</th>\n",
       "      <th>X16</th>\n",
       "      <th>X17</th>\n",
       "      <th>X18</th>\n",
       "      <th>X19</th>\n",
       "      <th>X20</th>\n",
       "      <th>X21</th>\n",
       "      <th>X22</th>\n",
       "      <th>X23</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID</td>\n",
       "      <td>LIMIT_BAL</td>\n",
       "      <td>SEX</td>\n",
       "      <td>EDUCATION</td>\n",
       "      <td>MARRIAGE</td>\n",
       "      <td>AGE</td>\n",
       "      <td>PAY_0</td>\n",
       "      <td>PAY_2</td>\n",
       "      <td>PAY_3</td>\n",
       "      <td>PAY_4</td>\n",
       "      <td>...</td>\n",
       "      <td>BILL_AMT4</td>\n",
       "      <td>BILL_AMT5</td>\n",
       "      <td>BILL_AMT6</td>\n",
       "      <td>PAY_AMT1</td>\n",
       "      <td>PAY_AMT2</td>\n",
       "      <td>PAY_AMT3</td>\n",
       "      <td>PAY_AMT4</td>\n",
       "      <td>PAY_AMT5</td>\n",
       "      <td>PAY_AMT6</td>\n",
       "      <td>default payment next month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>120000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272</td>\n",
       "      <td>3455</td>\n",
       "      <td>3261</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>90000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331</td>\n",
       "      <td>14948</td>\n",
       "      <td>15549</td>\n",
       "      <td>1518</td>\n",
       "      <td>1500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314</td>\n",
       "      <td>28959</td>\n",
       "      <td>29547</td>\n",
       "      <td>2000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1200</td>\n",
       "      <td>1100</td>\n",
       "      <td>1069</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0         X1   X2         X3        X4   X5     X6     X7     X8  \\\n",
       "0         ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3   \n",
       "1          1      20000    2          2         1   24      2      2     -1   \n",
       "2          2     120000    2          2         2   26     -1      2      0   \n",
       "3          3      90000    2          2         2   34      0      0      0   \n",
       "4          4      50000    2          2         1   37      0      0      0   \n",
       "\n",
       "      X9  ...        X15        X16        X17       X18       X19       X20  \\\n",
       "0  PAY_4  ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3   \n",
       "1     -1  ...          0          0          0         0       689         0   \n",
       "2      0  ...       3272       3455       3261         0      1000      1000   \n",
       "3      0  ...      14331      14948      15549      1518      1500      1000   \n",
       "4      0  ...      28314      28959      29547      2000      2019      1200   \n",
       "\n",
       "        X21       X22       X23                           Y  \n",
       "0  PAY_AMT4  PAY_AMT5  PAY_AMT6  default payment next month  \n",
       "1         0         0         0                           1  \n",
       "2      1000         0      2000                           1  \n",
       "3      1000      1000      5000                           0  \n",
       "4      1100      1069      1000                           0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "380f45c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>X11</th>\n",
       "      <th>X12</th>\n",
       "      <th>X13</th>\n",
       "      <th>...</th>\n",
       "      <th>X3_1</th>\n",
       "      <th>X3_2</th>\n",
       "      <th>X3_3</th>\n",
       "      <th>X3_4</th>\n",
       "      <th>X3_5</th>\n",
       "      <th>X3_6</th>\n",
       "      <th>X4_0</th>\n",
       "      <th>X4_1</th>\n",
       "      <th>X4_2</th>\n",
       "      <th>X4_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>3913.0</td>\n",
       "      <td>3102.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>120000.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2682.0</td>\n",
       "      <td>1725.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90000.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29239.0</td>\n",
       "      <td>14027.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46990.0</td>\n",
       "      <td>48233.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8617.0</td>\n",
       "      <td>5670.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64400.0</td>\n",
       "      <td>57069.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         X1    X5   X6   X7   X8   X9  X10  X11      X12      X13  ...  X3_1  \\\n",
       "1   20000.0  24.0  2.0  2.0 -1.0 -1.0 -2.0 -2.0   3913.0   3102.0  ...   0.0   \n",
       "2  120000.0  26.0 -1.0  2.0  0.0  0.0  0.0  2.0   2682.0   1725.0  ...   0.0   \n",
       "3   90000.0  34.0  0.0  0.0  0.0  0.0  0.0  0.0  29239.0  14027.0  ...   0.0   \n",
       "4   50000.0  37.0  0.0  0.0  0.0  0.0  0.0  0.0  46990.0  48233.0  ...   0.0   \n",
       "5   50000.0  57.0 -1.0  0.0 -1.0  0.0  0.0  0.0   8617.0   5670.0  ...   0.0   \n",
       "6   50000.0  37.0  0.0  0.0  0.0  0.0  0.0  0.0  64400.0  57069.0  ...   1.0   \n",
       "\n",
       "   X3_2  X3_3  X3_4  X3_5  X3_6  X4_0  X4_1  X4_2  X4_3  \n",
       "1   1.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0   0.0  \n",
       "2   1.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0  \n",
       "3   1.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0  \n",
       "4   1.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0   0.0  \n",
       "5   1.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0   0.0  \n",
       "6   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0  \n",
       "\n",
       "[6 rows x 34 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(data.iloc[1:,1:], columns=['X2','X3','X4'], drop_first=False).astype(float).head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cf4473b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(df):\n",
    "    # remove unnecessary 1st column and columne names in 1st row\n",
    "    data = df.iloc[1:, 1:].copy()\n",
    "    # one-hot encoding\n",
    "    categorical_cols = ['X2','X3','X4']\n",
    "    ohe = OneHotEncoder(sparse_output=False, drop=None)\n",
    "    ohe_array = ohe.fit_transform(data[categorical_cols])\n",
    "    ohe_array = ohe_array.astype(int) # converting booleans to 0/1\n",
    "    ohe_cols = ohe.get_feature_names_out(categorical_cols)\n",
    "    # merge ohe with original data\n",
    "    data_ohe = pd.DataFrame(ohe_array, columns=ohe_cols, index=data.index)\n",
    "    # remove original categorical cols\n",
    "    data_final = pd.concat([data.drop(columns=categorical_cols), data_ohe], axis=1)\n",
    "    # convert target col to 0/1\n",
    "    data_final['Y'] = data_final['Y'].astype(int)\n",
    "\n",
    "    return data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "974753d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30000 entries, 1 to 30000\n",
      "Data columns (total 34 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   X1      30000 non-null  object\n",
      " 1   X5      30000 non-null  object\n",
      " 2   X6      30000 non-null  object\n",
      " 3   X7      30000 non-null  object\n",
      " 4   X8      30000 non-null  object\n",
      " 5   X9      30000 non-null  object\n",
      " 6   X10     30000 non-null  object\n",
      " 7   X11     30000 non-null  object\n",
      " 8   X12     30000 non-null  object\n",
      " 9   X13     30000 non-null  object\n",
      " 10  X14     30000 non-null  object\n",
      " 11  X15     30000 non-null  object\n",
      " 12  X16     30000 non-null  object\n",
      " 13  X17     30000 non-null  object\n",
      " 14  X18     30000 non-null  object\n",
      " 15  X19     30000 non-null  object\n",
      " 16  X20     30000 non-null  object\n",
      " 17  X21     30000 non-null  object\n",
      " 18  X22     30000 non-null  object\n",
      " 19  X23     30000 non-null  object\n",
      " 20  Y       30000 non-null  int64 \n",
      " 21  X2_1    30000 non-null  int64 \n",
      " 22  X2_2    30000 non-null  int64 \n",
      " 23  X3_0    30000 non-null  int64 \n",
      " 24  X3_1    30000 non-null  int64 \n",
      " 25  X3_2    30000 non-null  int64 \n",
      " 26  X3_3    30000 non-null  int64 \n",
      " 27  X3_4    30000 non-null  int64 \n",
      " 28  X3_5    30000 non-null  int64 \n",
      " 29  X3_6    30000 non-null  int64 \n",
      " 30  X4_0    30000 non-null  int64 \n",
      " 31  X4_1    30000 non-null  int64 \n",
      " 32  X4_2    30000 non-null  int64 \n",
      " 33  X4_3    30000 non-null  int64 \n",
      "dtypes: int64(14), object(20)\n",
      "memory usage: 7.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# prepare data\n",
    "prep_data = data_prep(data)\n",
    "prep_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500c1968",
   "metadata": {},
   "source": [
    "#### for handling imbalanced class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c22ee95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-computing sample weights once and for all\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "sample_weights = compute_sample_weight(class_weight='balanced', y=prep_data['Y'])\n",
    "X = prep_data.drop('Y', axis=1)\n",
    "y = prep_data['Y'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c151658",
   "metadata": {},
   "source": [
    "#### Feature Selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d9ef1892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Correlation-based Important Features\n",
      "-----------------------------------\n",
      "['X1', 'X11', 'X10', 'X9', 'X8', 'X7', 'X6']\n",
      "\n",
      "-----------------------------------\n",
      "LASSO-based Important Features\n",
      "-----------------------------------\n",
      "['X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X18', 'X2_2', 'X3_0', 'X3_1', 'X3_2', 'X3_3', 'X3_4', 'X3_5', 'X4_0', 'X4_1', 'X4_2']\n",
      "\n",
      "-----------------------------------\n",
      "Tree-based Important Features\n",
      "-----------------------------------\n",
      "['X1', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12', 'X13', 'X14', 'X15', 'X16', 'X17', 'X18', 'X19', 'X20', 'X21', 'X22', 'X23']\n",
      "\n",
      "Number of PCA components for 99% variance: 14\n",
      "\n",
      "-----------------------------------\n",
      "RFE-based Important Features\n",
      "-----------------------------------\n",
      "['X1', 'X5', 'X6', 'X12', 'X13', 'X14', 'X16', 'X17', 'X18', 'X19', 'X20', 'X21', 'X22', 'X23']\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 1. Pearson (Correlation-based selection)\n",
    "# -----------------------------------------------------------\n",
    "corr_df = pd.DataFrame(X.join(y).corr()['Y'].sort_values())\n",
    "corr_df['abs'] = corr_df['Y'].abs()\n",
    "\n",
    "corr_features = list(\n",
    "    corr_df.loc[(corr_df['abs'] >= 0.08) & (corr_df.index != 'Y')].index\n",
    ")\n",
    "\n",
    "print(\"-----------------------------------\")\n",
    "print(\"Correlation-based Important Features\")\n",
    "print(\"-----------------------------------\")\n",
    "print(corr_features)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 2. LASSO (L1 Regularization)\n",
    "# -----------------------------------------------------------\n",
    "lr_l1 = LogisticRegression(\n",
    "    penalty='l1',\n",
    "    C=1,\n",
    "    solver='liblinear',\n",
    "    max_iter=500\n",
    ")\n",
    "\n",
    "lr_l1.fit(X, y, sample_weight=sample_weights)\n",
    "\n",
    "sfm_l1 = SelectFromModel(lr_l1, prefit=True)\n",
    "lasso_features = list(X.columns[sfm_l1.get_support()])\n",
    "\n",
    "print(\"\\n-----------------------------------\")\n",
    "print(\"LASSO-based Important Features\")\n",
    "print(\"-----------------------------------\")\n",
    "print(lasso_features)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 3. Tree-Based (ExtraTrees)\n",
    "# -----------------------------------------------------------\n",
    "tree_model = ExtraTreesClassifier(\n",
    "    n_estimators=200,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "tree_model.fit(X, y, sample_weight=sample_weights)\n",
    "\n",
    "sfm_tree = SelectFromModel(tree_model, prefit=True)\n",
    "tree_features = list(X.columns[sfm_tree.get_support()])\n",
    "\n",
    "print(\"\\n-----------------------------------\")\n",
    "print(\"Tree-based Important Features\")\n",
    "print(\"-----------------------------------\")\n",
    "print(tree_features)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 4. PCA → RFE\n",
    "# -----------------------------------------------------------\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=0.99)\n",
    "X_reduced = pca.fit_transform(X_scaled)\n",
    "\n",
    "N = pca.n_components_\n",
    "print(\"\\nNumber of PCA components for 99% variance:\", N)\n",
    "\n",
    "rfe = RFE(\n",
    "    estimator=DecisionTreeClassifier(),\n",
    "    n_features_to_select=N\n",
    ")\n",
    "\n",
    "rfe.fit(X, y, sample_weight=sample_weights)\n",
    "\n",
    "rfe_features = list(X.columns[rfe.support_])\n",
    "\n",
    "print(\"\\n-----------------------------------\")\n",
    "print(\"RFE-based Important Features\")\n",
    "print(\"-----------------------------------\")\n",
    "print(rfe_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eef6666",
   "metadata": {},
   "source": [
    "#### Creting subsets from original dataset basis each feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f1e0fdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# append target variable to form complete set cols to \n",
    "# for creating multiple datasets for training & benchmarking\n",
    "corr_features.append('Y')\n",
    "lasso_features.append('Y')\n",
    "tree_features.append('Y')\n",
    "rfe_features.append('Y')\n",
    "all_features = X.columns.tolist()\n",
    "all_features.append('Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "404ef895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X1', 'X11', 'X10', 'X9', 'X8', 'X7', 'X6', 'Y']\n",
      "['X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X18', 'X2_2', 'X3_0', 'X3_1', 'X3_2', 'X3_3', 'X3_4', 'X3_5', 'X4_0', 'X4_1', 'X4_2', 'Y']\n",
      "['X1', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12', 'X13', 'X14', 'X15', 'X16', 'X17', 'X18', 'X19', 'X20', 'X21', 'X22', 'X23', 'Y']\n",
      "['X1', 'X5', 'X6', 'X12', 'X13', 'X14', 'X16', 'X17', 'X18', 'X19', 'X20', 'X21', 'X22', 'X23', 'Y']\n",
      "['X1', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12', 'X13', 'X14', 'X15', 'X16', 'X17', 'X18', 'X19', 'X20', 'X21', 'X22', 'X23', 'X2_1', 'X2_2', 'X3_0', 'X3_1', 'X3_2', 'X3_3', 'X3_4', 'X3_5', 'X3_6', 'X4_0', 'X4_1', 'X4_2', 'X4_3', 'Y']\n"
     ]
    }
   ],
   "source": [
    "print(corr_features)\n",
    "print(lasso_features)\n",
    "print(tree_features)\n",
    "print(rfe_features)\n",
    "print(all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f46734f",
   "metadata": {},
   "source": [
    "#### Benchmarking multiple classifiers (algos) across multiple feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5bf994af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Feature subsets \n",
    "# ============================================================\n",
    "\n",
    "feature_sets = [\n",
    "    corr_features,\n",
    "    lasso_features,\n",
    "    tree_features,\n",
    "    rfe_features,\n",
    "    all_features      \n",
    "]\n",
    "\n",
    "dataset_names = [\n",
    "    'corr_feat_dataset',\n",
    "    'lasso_feat_dataset',\n",
    "    'tree_feat_dataset',\n",
    "    'rfe_feat_dataset',\n",
    "    'all_feat_dataset'\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# Creating datasets for each feature set\n",
    "# ============================================================\n",
    "\n",
    "datasets = []\n",
    "for fs in feature_sets:\n",
    "    ds = prep_data.loc[:, fs].reset_index(drop=True)\n",
    "    datasets.append(ds)\n",
    "\n",
    "# ============================================================\n",
    "# Queueing multiple Classifiers for running against mutiple FS\n",
    "# ============================================================\n",
    "\n",
    "names = [\n",
    "    \"Nearest Neighbors\",\n",
    "    \"Logistic Regression\",\n",
    "    # \"RBF SVM\",\n",
    "    \"Decision Tree\",\n",
    "    \"Random Forest\",\n",
    "    \"MLP Neural Net\",\n",
    "    \"AdaBoost\",\n",
    "    \"Gaussian NB\",\n",
    "    \"Quadratic DA\",\n",
    "    \"XGBoost\",\n",
    "    \"LightGBM\"\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(n_neighbors=10, p=1, weights='distance'),\n",
    "    LogisticRegression(max_iter=1000, class_weight='balanced'),\n",
    "    # SVC(kernel=\"rbf\", gamma=2, C=1, probability=True, class_weight='balanced'),       # <--- takes lot of time; uncomment only if you've patience\n",
    "    DecisionTreeClassifier(max_depth=5, class_weight='balanced'),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=100, class_weight='balanced'),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis(reg_param=0.1),\n",
    "    XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='logloss',\n",
    "        scale_pos_weight=(y.value_counts()[0] / y.value_counts()[1])  # for class balancing\n",
    "    ),\n",
    "    LGBMClassifier()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "33abbc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Dataset: corr_feat_dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Algorithms:  90%|█████████ | 9/10 [00:45<00:04,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4645, number of negative: 16355\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000630 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 141\n",
      "[LightGBM] [Info] Number of data points in the train set: 21000, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221190 -> initscore=-1.258742\n",
      "[LightGBM] [Info] Start training from score -1.258742\n",
      "[LightGBM] [Info] Number of positive: 3716, number of negative: 13084\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000481 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 137\n",
      "[LightGBM] [Info] Number of data points in the train set: 16800, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221190 -> initscore=-1.258742\n",
      "[LightGBM] [Info] Start training from score -1.258742\n",
      "[LightGBM] [Info] Number of positive: 3716, number of negative: 13084\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001108 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 139\n",
      "[LightGBM] [Info] Number of data points in the train set: 16800, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221190 -> initscore=-1.258742\n",
      "[LightGBM] [Info] Start training from score -1.258742\n",
      "[LightGBM] [Info] Number of positive: 3716, number of negative: 13084\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000484 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 136\n",
      "[LightGBM] [Info] Number of data points in the train set: 16800, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221190 -> initscore=-1.258742\n",
      "[LightGBM] [Info] Start training from score -1.258742\n",
      "[LightGBM] [Info] Number of positive: 3716, number of negative: 13084\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000555 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 139\n",
      "[LightGBM] [Info] Number of data points in the train set: 16800, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221190 -> initscore=-1.258742\n",
      "[LightGBM] [Info] Start training from score -1.258742\n",
      "[LightGBM] [Info] Number of positive: 3716, number of negative: 13084\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000460 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 137\n",
      "[LightGBM] [Info] Number of data points in the train set: 16800, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221190 -> initscore=-1.258742\n",
      "[LightGBM] [Info] Start training from score -1.258742\n",
      "[LightGBM] [Info] Number of positive: 1593, number of negative: 5607\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000237 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 127\n",
      "[LightGBM] [Info] Number of data points in the train set: 7200, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221250 -> initscore=-1.258397\n",
      "[LightGBM] [Info] Start training from score -1.258397\n",
      "[LightGBM] [Info] Number of positive: 1593, number of negative: 5607\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000491 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 127\n",
      "[LightGBM] [Info] Number of data points in the train set: 7200, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221250 -> initscore=-1.258397\n",
      "[LightGBM] [Info] Start training from score -1.258397\n",
      "[LightGBM] [Info] Number of positive: 1593, number of negative: 5607\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000887 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 126\n",
      "[LightGBM] [Info] Number of data points in the train set: 7200, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221250 -> initscore=-1.258397\n",
      "[LightGBM] [Info] Start training from score -1.258397\n",
      "[LightGBM] [Info] Number of positive: 1593, number of negative: 5607\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000251 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 130\n",
      "[LightGBM] [Info] Number of data points in the train set: 7200, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221250 -> initscore=-1.258397\n",
      "[LightGBM] [Info] Start training from score -1.258397\n",
      "[LightGBM] [Info] Number of positive: 1592, number of negative: 5608\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000229 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 122\n",
      "[LightGBM] [Info] Number of data points in the train set: 7200, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221111 -> initscore=-1.259203\n",
      "[LightGBM] [Info] Start training from score -1.259203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Algorithms: 100%|██████████| 10/10 [00:48<00:00,  4.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Dataset: lasso_feat_dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Algorithms:  90%|█████████ | 9/10 [01:03<00:05,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4645, number of negative: 16355\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001614 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 404\n",
      "[LightGBM] [Info] Number of data points in the train set: 21000, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221190 -> initscore=-1.258742\n",
      "[LightGBM] [Info] Start training from score -1.258742\n",
      "[LightGBM] [Info] Number of positive: 3716, number of negative: 13084\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002339 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 401\n",
      "[LightGBM] [Info] Number of data points in the train set: 16800, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221190 -> initscore=-1.258742\n",
      "[LightGBM] [Info] Start training from score -1.258742\n",
      "[LightGBM] [Info] Number of positive: 3716, number of negative: 13084\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001795 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 403\n",
      "[LightGBM] [Info] Number of data points in the train set: 16800, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221190 -> initscore=-1.258742\n",
      "[LightGBM] [Info] Start training from score -1.258742\n",
      "[LightGBM] [Info] Number of positive: 3716, number of negative: 13084\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002089 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 402\n",
      "[LightGBM] [Info] Number of data points in the train set: 16800, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221190 -> initscore=-1.258742\n",
      "[LightGBM] [Info] Start training from score -1.258742\n",
      "[LightGBM] [Info] Number of positive: 3716, number of negative: 13084\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001942 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 403\n",
      "[LightGBM] [Info] Number of data points in the train set: 16800, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221190 -> initscore=-1.258742\n",
      "[LightGBM] [Info] Start training from score -1.258742\n",
      "[LightGBM] [Info] Number of positive: 3716, number of negative: 13084\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001167 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 401\n",
      "[LightGBM] [Info] Number of data points in the train set: 16800, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221190 -> initscore=-1.258742\n",
      "[LightGBM] [Info] Start training from score -1.258742\n",
      "[LightGBM] [Info] Number of positive: 1593, number of negative: 5607\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000555 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 392\n",
      "[LightGBM] [Info] Number of data points in the train set: 7200, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221250 -> initscore=-1.258397\n",
      "[LightGBM] [Info] Start training from score -1.258397\n",
      "[LightGBM] [Info] Number of positive: 1593, number of negative: 5607\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000564 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 392\n",
      "[LightGBM] [Info] Number of data points in the train set: 7200, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221250 -> initscore=-1.258397\n",
      "[LightGBM] [Info] Start training from score -1.258397\n",
      "[LightGBM] [Info] Number of positive: 1593, number of negative: 5607\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000582 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 392\n",
      "[LightGBM] [Info] Number of data points in the train set: 7200, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221250 -> initscore=-1.258397\n",
      "[LightGBM] [Info] Start training from score -1.258397\n",
      "[LightGBM] [Info] Number of positive: 1593, number of negative: 5607\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000656 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 395\n",
      "[LightGBM] [Info] Number of data points in the train set: 7200, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221250 -> initscore=-1.258397\n",
      "[LightGBM] [Info] Start training from score -1.258397\n",
      "[LightGBM] [Info] Number of positive: 1592, number of negative: 5608\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000555 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 388\n",
      "[LightGBM] [Info] Number of data points in the train set: 7200, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221111 -> initscore=-1.259203\n",
      "[LightGBM] [Info] Start training from score -1.259203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Algorithms: 100%|██████████| 10/10 [01:05<00:00,  6.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Dataset: tree_feat_dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Algorithms:  90%|█████████ | 9/10 [01:27<00:07,  7.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4645, number of negative: 16355\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004789 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3255\n",
      "[LightGBM] [Info] Number of data points in the train set: 21000, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221190 -> initscore=-1.258742\n",
      "[LightGBM] [Info] Start training from score -1.258742\n",
      "[LightGBM] [Info] Number of positive: 3716, number of negative: 13084\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003635 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3250\n",
      "[LightGBM] [Info] Number of data points in the train set: 16800, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221190 -> initscore=-1.258742\n",
      "[LightGBM] [Info] Start training from score -1.258742\n",
      "[LightGBM] [Info] Number of positive: 3716, number of negative: 13084\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003674 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3253\n",
      "[LightGBM] [Info] Number of data points in the train set: 16800, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221190 -> initscore=-1.258742\n",
      "[LightGBM] [Info] Start training from score -1.258742\n",
      "[LightGBM] [Info] Number of positive: 3716, number of negative: 13084\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003525 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3250\n",
      "[LightGBM] [Info] Number of data points in the train set: 16800, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221190 -> initscore=-1.258742\n",
      "[LightGBM] [Info] Start training from score -1.258742\n",
      "[LightGBM] [Info] Number of positive: 3716, number of negative: 13084\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003540 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3252\n",
      "[LightGBM] [Info] Number of data points in the train set: 16800, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221190 -> initscore=-1.258742\n",
      "[LightGBM] [Info] Start training from score -1.258742\n",
      "[LightGBM] [Info] Number of positive: 3716, number of negative: 13084\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003492 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3249\n",
      "[LightGBM] [Info] Number of data points in the train set: 16800, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221190 -> initscore=-1.258742\n",
      "[LightGBM] [Info] Start training from score -1.258742\n",
      "[LightGBM] [Info] Number of positive: 1593, number of negative: 5607\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001531 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3238\n",
      "[LightGBM] [Info] Number of data points in the train set: 7200, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221250 -> initscore=-1.258397\n",
      "[LightGBM] [Info] Start training from score -1.258397\n",
      "[LightGBM] [Info] Number of positive: 1593, number of negative: 5607\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001551 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3237\n",
      "[LightGBM] [Info] Number of data points in the train set: 7200, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221250 -> initscore=-1.258397\n",
      "[LightGBM] [Info] Start training from score -1.258397\n",
      "[LightGBM] [Info] Number of positive: 1593, number of negative: 5607\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001606 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3237\n",
      "[LightGBM] [Info] Number of data points in the train set: 7200, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221250 -> initscore=-1.258397\n",
      "[LightGBM] [Info] Start training from score -1.258397\n",
      "[LightGBM] [Info] Number of positive: 1593, number of negative: 5607\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001692 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3242\n",
      "[LightGBM] [Info] Number of data points in the train set: 7200, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221250 -> initscore=-1.258397\n",
      "[LightGBM] [Info] Start training from score -1.258397\n",
      "[LightGBM] [Info] Number of positive: 1592, number of negative: 5608\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001870 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3233\n",
      "[LightGBM] [Info] Number of data points in the train set: 7200, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221111 -> initscore=-1.259203\n",
      "[LightGBM] [Info] Start training from score -1.259203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Algorithms: 100%|██████████| 10/10 [01:32<00:00,  9.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Dataset: rfe_feat_dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Algorithms:  90%|█████████ | 9/10 [01:10<00:05,  5.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4645, number of negative: 16355\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002208 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2944\n",
      "[LightGBM] [Info] Number of data points in the train set: 21000, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221190 -> initscore=-1.258742\n",
      "[LightGBM] [Info] Start training from score -1.258742\n",
      "[LightGBM] [Info] Number of positive: 3716, number of negative: 13084\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001672 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2941\n",
      "[LightGBM] [Info] Number of data points in the train set: 16800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221190 -> initscore=-1.258742\n",
      "[LightGBM] [Info] Start training from score -1.258742\n",
      "[LightGBM] [Info] Number of positive: 3716, number of negative: 13084\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001531 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2943\n",
      "[LightGBM] [Info] Number of data points in the train set: 16800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221190 -> initscore=-1.258742\n",
      "[LightGBM] [Info] Start training from score -1.258742\n",
      "[LightGBM] [Info] Number of positive: 3716, number of negative: 13084\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001607 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2941\n",
      "[LightGBM] [Info] Number of data points in the train set: 16800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221190 -> initscore=-1.258742\n",
      "[LightGBM] [Info] Start training from score -1.258742\n",
      "[LightGBM] [Info] Number of positive: 3716, number of negative: 13084\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011304 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2941\n",
      "[LightGBM] [Info] Number of data points in the train set: 16800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221190 -> initscore=-1.258742\n",
      "[LightGBM] [Info] Start training from score -1.258742\n",
      "[LightGBM] [Info] Number of positive: 3716, number of negative: 13084\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001720 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2939\n",
      "[LightGBM] [Info] Number of data points in the train set: 16800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221190 -> initscore=-1.258742\n",
      "[LightGBM] [Info] Start training from score -1.258742\n",
      "[LightGBM] [Info] Number of positive: 1593, number of negative: 5607\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000871 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2932\n",
      "[LightGBM] [Info] Number of data points in the train set: 7200, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221250 -> initscore=-1.258397\n",
      "[LightGBM] [Info] Start training from score -1.258397\n",
      "[LightGBM] [Info] Number of positive: 1593, number of negative: 5607\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000815 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2930\n",
      "[LightGBM] [Info] Number of data points in the train set: 7200, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221250 -> initscore=-1.258397\n",
      "[LightGBM] [Info] Start training from score -1.258397\n",
      "[LightGBM] [Info] Number of positive: 1593, number of negative: 5607\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000850 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2930\n",
      "[LightGBM] [Info] Number of data points in the train set: 7200, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221250 -> initscore=-1.258397\n",
      "[LightGBM] [Info] Start training from score -1.258397\n",
      "[LightGBM] [Info] Number of positive: 1593, number of negative: 5607\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000574 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2934\n",
      "[LightGBM] [Info] Number of data points in the train set: 7200, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221250 -> initscore=-1.258397\n",
      "[LightGBM] [Info] Start training from score -1.258397\n",
      "[LightGBM] [Info] Number of positive: 1592, number of negative: 5608\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000693 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2931\n",
      "[LightGBM] [Info] Number of data points in the train set: 7200, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221111 -> initscore=-1.259203\n",
      "[LightGBM] [Info] Start training from score -1.259203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Algorithms: 100%|██████████| 10/10 [01:13<00:00,  7.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Dataset: all_feat_dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Algorithms:  90%|█████████ | 9/10 [01:43<00:08,  8.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4645, number of negative: 16355\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005180 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3291\n",
      "[LightGBM] [Info] Number of data points in the train set: 21000, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221190 -> initscore=-1.258742\n",
      "[LightGBM] [Info] Start training from score -1.258742\n",
      "[LightGBM] [Info] Number of positive: 3716, number of negative: 13084\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003318 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3286\n",
      "[LightGBM] [Info] Number of data points in the train set: 16800, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221190 -> initscore=-1.258742\n",
      "[LightGBM] [Info] Start training from score -1.258742\n",
      "[LightGBM] [Info] Number of positive: 3716, number of negative: 13084\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004145 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3289\n",
      "[LightGBM] [Info] Number of data points in the train set: 16800, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221190 -> initscore=-1.258742\n",
      "[LightGBM] [Info] Start training from score -1.258742\n",
      "[LightGBM] [Info] Number of positive: 3716, number of negative: 13084\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003936 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3286\n",
      "[LightGBM] [Info] Number of data points in the train set: 16800, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221190 -> initscore=-1.258742\n",
      "[LightGBM] [Info] Start training from score -1.258742\n",
      "[LightGBM] [Info] Number of positive: 3716, number of negative: 13084\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003231 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3288\n",
      "[LightGBM] [Info] Number of data points in the train set: 16800, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221190 -> initscore=-1.258742\n",
      "[LightGBM] [Info] Start training from score -1.258742\n",
      "[LightGBM] [Info] Number of positive: 3716, number of negative: 13084\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003294 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3285\n",
      "[LightGBM] [Info] Number of data points in the train set: 16800, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221190 -> initscore=-1.258742\n",
      "[LightGBM] [Info] Start training from score -1.258742\n",
      "[LightGBM] [Info] Number of positive: 1593, number of negative: 5607\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001761 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3268\n",
      "[LightGBM] [Info] Number of data points in the train set: 7200, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221250 -> initscore=-1.258397\n",
      "[LightGBM] [Info] Start training from score -1.258397\n",
      "[LightGBM] [Info] Number of positive: 1593, number of negative: 5607\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001679 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3267\n",
      "[LightGBM] [Info] Number of data points in the train set: 7200, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221250 -> initscore=-1.258397\n",
      "[LightGBM] [Info] Start training from score -1.258397\n",
      "[LightGBM] [Info] Number of positive: 1593, number of negative: 5607\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001903 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3267\n",
      "[LightGBM] [Info] Number of data points in the train set: 7200, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221250 -> initscore=-1.258397\n",
      "[LightGBM] [Info] Start training from score -1.258397\n",
      "[LightGBM] [Info] Number of positive: 1593, number of negative: 5607\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001660 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3272\n",
      "[LightGBM] [Info] Number of data points in the train set: 7200, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221250 -> initscore=-1.258397\n",
      "[LightGBM] [Info] Start training from score -1.258397\n",
      "[LightGBM] [Info] Number of positive: 1592, number of negative: 5608\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004954 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3263\n",
      "[LightGBM] [Info] Number of data points in the train set: 7200, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221111 -> initscore=-1.259203\n",
      "[LightGBM] [Info] Start training from score -1.259203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Algorithms: 100%|██████████| 10/10 [01:48<00:00, 10.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Benchmarking complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# Full Benchmarking across Feature Sets and various Classifiers\n",
    "# ================================================================\n",
    "\n",
    "modelling_df = pd.DataFrame(\n",
    "    columns=[\n",
    "        'feats_subset', 'algo',\n",
    "        'train-precision', 'train-recall', 'train-f1', 'train-acc',\n",
    "        'val-precision', 'val-recall', 'val-f1', 'val-acc'\n",
    "    ]\n",
    ")\n",
    "\n",
    "i = 0\n",
    "\n",
    "for ds_cnt in range(len(datasets)):\n",
    "    print(f\"\\n Dataset: {dataset_names[ds_cnt]}\")\n",
    "    ds = datasets[ds_cnt]\n",
    "\n",
    "    X_sub = ds.drop(columns=['Y'])\n",
    "    y_sub = ds['Y']\n",
    "\n",
    "    # Standardize numeric features only\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_sub)\n",
    "\n",
    "    # Stratified train - test dataset split\n",
    "    sss = StratifiedShuffleSplit(n_splits=4, test_size=0.3, random_state=29)\n",
    "    for train_idx, val_idx in sss.split(X_scaled, y_sub):\n",
    "        X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]\n",
    "        y_train, y_val = y_sub.iloc[train_idx], y_sub.iloc[val_idx]\n",
    "        \n",
    "        y_train = y_train.squeeze().astype(int)\n",
    "        y_val = y_val.squeeze().astype(int)\n",
    "\n",
    "    # Benchmark models\n",
    "    for clf_cnt in tqdm(range(len(classifiers)), desc=\"Algorithms\", position=0):\n",
    "        \n",
    "        clf = classifiers[clf_cnt]\n",
    "        name = names[clf_cnt]\n",
    "\n",
    "        modelling_df.at[i, 'feats_subset'] = dataset_names[ds_cnt]\n",
    "        modelling_df.at[i, 'algo'] = name\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Train CV\n",
    "        y_pred_train = cross_val_predict(clf, X_train, y_train, cv=5, method=\"predict\")\n",
    "\n",
    "        modelling_df.at[i, 'train-precision'] = precision_score(y_train, y_pred_train)\n",
    "        modelling_df.at[i, 'train-recall'] = recall_score(y_train, y_pred_train)\n",
    "        modelling_df.at[i, 'train-f1'] = f1_score(y_train, y_pred_train)\n",
    "        modelling_df.at[i, 'train-acc'] = accuracy_score(y_train, y_pred_train)\n",
    "\n",
    "        # Validation CV\n",
    "        y_pred_val = cross_val_predict(clf, X_val, y_val, cv=5, method=\"predict\")\n",
    "\n",
    "        modelling_df.at[i, 'val-precision'] = precision_score(y_val, y_pred_val)\n",
    "        modelling_df.at[i, 'val-recall'] = recall_score(y_val, y_pred_val)\n",
    "        modelling_df.at[i, 'val-f1'] = f1_score(y_val, y_pred_val)\n",
    "        modelling_df.at[i, 'val-acc'] = accuracy_score(y_val, y_pred_val)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "print(\"\\n Benchmarking complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "86674eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feats_subset</th>\n",
       "      <th>algo</th>\n",
       "      <th>train-precision</th>\n",
       "      <th>train-recall</th>\n",
       "      <th>train-f1</th>\n",
       "      <th>train-acc</th>\n",
       "      <th>val-precision</th>\n",
       "      <th>val-recall</th>\n",
       "      <th>val-f1</th>\n",
       "      <th>val-acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>all_feat_dataset</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.485351</td>\n",
       "      <td>0.599139</td>\n",
       "      <td>0.536275</td>\n",
       "      <td>0.77081</td>\n",
       "      <td>0.514706</td>\n",
       "      <td>0.59769</td>\n",
       "      <td>0.553102</td>\n",
       "      <td>0.786333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>tree_feat_dataset</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.4913</td>\n",
       "      <td>0.589666</td>\n",
       "      <td>0.536008</td>\n",
       "      <td>0.77419</td>\n",
       "      <td>0.512364</td>\n",
       "      <td>0.593169</td>\n",
       "      <td>0.549814</td>\n",
       "      <td>0.785111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>corr_feat_dataset</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.50257</td>\n",
       "      <td>0.568353</td>\n",
       "      <td>0.533441</td>\n",
       "      <td>0.780095</td>\n",
       "      <td>0.523481</td>\n",
       "      <td>0.57107</td>\n",
       "      <td>0.546241</td>\n",
       "      <td>0.790111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>corr_feat_dataset</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.500469</td>\n",
       "      <td>0.573735</td>\n",
       "      <td>0.534604</td>\n",
       "      <td>0.779048</td>\n",
       "      <td>0.501485</td>\n",
       "      <td>0.593672</td>\n",
       "      <td>0.543698</td>\n",
       "      <td>0.779556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>lasso_feat_dataset</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.498963</td>\n",
       "      <td>0.569645</td>\n",
       "      <td>0.531966</td>\n",
       "      <td>0.778286</td>\n",
       "      <td>0.520441</td>\n",
       "      <td>0.569061</td>\n",
       "      <td>0.543666</td>\n",
       "      <td>0.788667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>rfe_feat_dataset</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.459343</td>\n",
       "      <td>0.59591</td>\n",
       "      <td>0.518789</td>\n",
       "      <td>0.755476</td>\n",
       "      <td>0.492687</td>\n",
       "      <td>0.592165</td>\n",
       "      <td>0.537865</td>\n",
       "      <td>0.774889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>lasso_feat_dataset</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.498018</td>\n",
       "      <td>0.567922</td>\n",
       "      <td>0.530678</td>\n",
       "      <td>0.77781</td>\n",
       "      <td>0.492575</td>\n",
       "      <td>0.583124</td>\n",
       "      <td>0.534039</td>\n",
       "      <td>0.774889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>tree_feat_dataset</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.458018</td>\n",
       "      <td>0.586006</td>\n",
       "      <td>0.514167</td>\n",
       "      <td>0.755048</td>\n",
       "      <td>0.459376</td>\n",
       "      <td>0.599196</td>\n",
       "      <td>0.520052</td>\n",
       "      <td>0.755333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>all_feat_dataset</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.457835</td>\n",
       "      <td>0.585576</td>\n",
       "      <td>0.513886</td>\n",
       "      <td>0.754952</td>\n",
       "      <td>0.459168</td>\n",
       "      <td>0.598694</td>\n",
       "      <td>0.51973</td>\n",
       "      <td>0.755222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>corr_feat_dataset</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.468478</td>\n",
       "      <td>0.567922</td>\n",
       "      <td>0.513429</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.460025</td>\n",
       "      <td>0.563536</td>\n",
       "      <td>0.506546</td>\n",
       "      <td>0.757111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>tree_feat_dataset</td>\n",
       "      <td>Gaussian NB</td>\n",
       "      <td>0.361323</td>\n",
       "      <td>0.681808</td>\n",
       "      <td>0.472334</td>\n",
       "      <td>0.663048</td>\n",
       "      <td>0.414977</td>\n",
       "      <td>0.637368</td>\n",
       "      <td>0.502674</td>\n",
       "      <td>0.721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>tree_feat_dataset</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.462034</td>\n",
       "      <td>0.555436</td>\n",
       "      <td>0.504448</td>\n",
       "      <td>0.758619</td>\n",
       "      <td>0.494163</td>\n",
       "      <td>0.510296</td>\n",
       "      <td>0.5021</td>\n",
       "      <td>0.776111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>corr_feat_dataset</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.355861</td>\n",
       "      <td>0.65296</td>\n",
       "      <td>0.460662</td>\n",
       "      <td>0.66181</td>\n",
       "      <td>0.417991</td>\n",
       "      <td>0.620794</td>\n",
       "      <td>0.499596</td>\n",
       "      <td>0.724889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>all_feat_dataset</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.468183</td>\n",
       "      <td>0.567061</td>\n",
       "      <td>0.5129</td>\n",
       "      <td>0.761762</td>\n",
       "      <td>0.490998</td>\n",
       "      <td>0.506781</td>\n",
       "      <td>0.498764</td>\n",
       "      <td>0.774667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>tree_feat_dataset</td>\n",
       "      <td>Quadratic DA</td>\n",
       "      <td>0.320565</td>\n",
       "      <td>0.753068</td>\n",
       "      <td>0.449701</td>\n",
       "      <td>0.592333</td>\n",
       "      <td>0.395578</td>\n",
       "      <td>0.664992</td>\n",
       "      <td>0.496066</td>\n",
       "      <td>0.701111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>rfe_feat_dataset</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.411739</td>\n",
       "      <td>0.601076</td>\n",
       "      <td>0.48871</td>\n",
       "      <td>0.72181</td>\n",
       "      <td>0.405848</td>\n",
       "      <td>0.634355</td>\n",
       "      <td>0.495003</td>\n",
       "      <td>0.713667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>lasso_feat_dataset</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.397337</td>\n",
       "      <td>0.610334</td>\n",
       "      <td>0.481324</td>\n",
       "      <td>0.709048</td>\n",
       "      <td>0.406645</td>\n",
       "      <td>0.614766</td>\n",
       "      <td>0.489502</td>\n",
       "      <td>0.716333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>lasso_feat_dataset</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.447779</td>\n",
       "      <td>0.575027</td>\n",
       "      <td>0.503487</td>\n",
       "      <td>0.749143</td>\n",
       "      <td>0.445873</td>\n",
       "      <td>0.53993</td>\n",
       "      <td>0.488414</td>\n",
       "      <td>0.749778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>corr_feat_dataset</td>\n",
       "      <td>Gaussian NB</td>\n",
       "      <td>0.572261</td>\n",
       "      <td>0.401507</td>\n",
       "      <td>0.471913</td>\n",
       "      <td>0.801238</td>\n",
       "      <td>0.573379</td>\n",
       "      <td>0.421899</td>\n",
       "      <td>0.486111</td>\n",
       "      <td>0.802667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>tree_feat_dataset</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.373769</td>\n",
       "      <td>0.645425</td>\n",
       "      <td>0.473393</td>\n",
       "      <td>0.682381</td>\n",
       "      <td>0.395768</td>\n",
       "      <td>0.629332</td>\n",
       "      <td>0.485941</td>\n",
       "      <td>0.705444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>all_feat_dataset</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.372537</td>\n",
       "      <td>0.651238</td>\n",
       "      <td>0.473952</td>\n",
       "      <td>0.680238</td>\n",
       "      <td>0.38911</td>\n",
       "      <td>0.638875</td>\n",
       "      <td>0.48365</td>\n",
       "      <td>0.698222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>rfe_feat_dataset</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.45879</td>\n",
       "      <td>0.55845</td>\n",
       "      <td>0.503738</td>\n",
       "      <td>0.756619</td>\n",
       "      <td>0.474477</td>\n",
       "      <td>0.490206</td>\n",
       "      <td>0.482213</td>\n",
       "      <td>0.767111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>lasso_feat_dataset</td>\n",
       "      <td>MLP Neural Net</td>\n",
       "      <td>0.684605</td>\n",
       "      <td>0.334123</td>\n",
       "      <td>0.449074</td>\n",
       "      <td>0.818667</td>\n",
       "      <td>0.687324</td>\n",
       "      <td>0.367654</td>\n",
       "      <td>0.479058</td>\n",
       "      <td>0.823111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>rfe_feat_dataset</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.367093</td>\n",
       "      <td>0.648439</td>\n",
       "      <td>0.468794</td>\n",
       "      <td>0.674952</td>\n",
       "      <td>0.390228</td>\n",
       "      <td>0.61778</td>\n",
       "      <td>0.47832</td>\n",
       "      <td>0.701889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>corr_feat_dataset</td>\n",
       "      <td>Quadratic DA</td>\n",
       "      <td>0.530036</td>\n",
       "      <td>0.440689</td>\n",
       "      <td>0.481251</td>\n",
       "      <td>0.789857</td>\n",
       "      <td>0.549175</td>\n",
       "      <td>0.41788</td>\n",
       "      <td>0.474615</td>\n",
       "      <td>0.795333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>corr_feat_dataset</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.668672</td>\n",
       "      <td>0.358881</td>\n",
       "      <td>0.467078</td>\n",
       "      <td>0.818857</td>\n",
       "      <td>0.646649</td>\n",
       "      <td>0.373179</td>\n",
       "      <td>0.473248</td>\n",
       "      <td>0.816222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>corr_feat_dataset</td>\n",
       "      <td>MLP Neural Net</td>\n",
       "      <td>0.679117</td>\n",
       "      <td>0.344456</td>\n",
       "      <td>0.457078</td>\n",
       "      <td>0.819</td>\n",
       "      <td>0.672862</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.472123</td>\n",
       "      <td>0.820111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>lasso_feat_dataset</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.672617</td>\n",
       "      <td>0.356943</td>\n",
       "      <td>0.466385</td>\n",
       "      <td>0.819333</td>\n",
       "      <td>0.647318</td>\n",
       "      <td>0.369663</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>corr_feat_dataset</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.684423</td>\n",
       "      <td>0.329171</td>\n",
       "      <td>0.444541</td>\n",
       "      <td>0.818048</td>\n",
       "      <td>0.680344</td>\n",
       "      <td>0.358112</td>\n",
       "      <td>0.469233</td>\n",
       "      <td>0.820778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>tree_feat_dataset</td>\n",
       "      <td>MLP Neural Net</td>\n",
       "      <td>0.68174</td>\n",
       "      <td>0.340797</td>\n",
       "      <td>0.454428</td>\n",
       "      <td>0.819</td>\n",
       "      <td>0.678811</td>\n",
       "      <td>0.3556</td>\n",
       "      <td>0.466711</td>\n",
       "      <td>0.820222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>tree_feat_dataset</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.663133</td>\n",
       "      <td>0.363617</td>\n",
       "      <td>0.469689</td>\n",
       "      <td>0.818381</td>\n",
       "      <td>0.636284</td>\n",
       "      <td>0.364641</td>\n",
       "      <td>0.463602</td>\n",
       "      <td>0.813333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>rfe_feat_dataset</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.69145</td>\n",
       "      <td>0.320344</td>\n",
       "      <td>0.43784</td>\n",
       "      <td>0.818048</td>\n",
       "      <td>0.700203</td>\n",
       "      <td>0.346057</td>\n",
       "      <td>0.463193</td>\n",
       "      <td>0.822556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>all_feat_dataset</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.668914</td>\n",
       "      <td>0.363186</td>\n",
       "      <td>0.470769</td>\n",
       "      <td>0.819381</td>\n",
       "      <td>0.640071</td>\n",
       "      <td>0.362632</td>\n",
       "      <td>0.462969</td>\n",
       "      <td>0.813889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>rfe_feat_dataset</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.670727</td>\n",
       "      <td>0.355651</td>\n",
       "      <td>0.464828</td>\n",
       "      <td>0.818857</td>\n",
       "      <td>0.64808</td>\n",
       "      <td>0.356102</td>\n",
       "      <td>0.459643</td>\n",
       "      <td>0.814778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>all_feat_dataset</td>\n",
       "      <td>MLP Neural Net</td>\n",
       "      <td>0.670628</td>\n",
       "      <td>0.340151</td>\n",
       "      <td>0.451364</td>\n",
       "      <td>0.817095</td>\n",
       "      <td>0.688064</td>\n",
       "      <td>0.34455</td>\n",
       "      <td>0.45917</td>\n",
       "      <td>0.820444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>lasso_feat_dataset</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.690625</td>\n",
       "      <td>0.333046</td>\n",
       "      <td>0.449383</td>\n",
       "      <td>0.819476</td>\n",
       "      <td>0.687375</td>\n",
       "      <td>0.34455</td>\n",
       "      <td>0.459016</td>\n",
       "      <td>0.820333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>all_feat_dataset</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.68678</td>\n",
       "      <td>0.32099</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.817429</td>\n",
       "      <td>0.698869</td>\n",
       "      <td>0.341537</td>\n",
       "      <td>0.458839</td>\n",
       "      <td>0.821778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>rfe_feat_dataset</td>\n",
       "      <td>MLP Neural Net</td>\n",
       "      <td>0.697381</td>\n",
       "      <td>0.30958</td>\n",
       "      <td>0.428806</td>\n",
       "      <td>0.817571</td>\n",
       "      <td>0.705696</td>\n",
       "      <td>0.336012</td>\n",
       "      <td>0.455257</td>\n",
       "      <td>0.822111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>tree_feat_dataset</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.684524</td>\n",
       "      <td>0.321851</td>\n",
       "      <td>0.437839</td>\n",
       "      <td>0.81719</td>\n",
       "      <td>0.69413</td>\n",
       "      <td>0.338523</td>\n",
       "      <td>0.455098</td>\n",
       "      <td>0.820667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>tree_feat_dataset</td>\n",
       "      <td>Nearest Neighbors</td>\n",
       "      <td>0.591589</td>\n",
       "      <td>0.357374</td>\n",
       "      <td>0.445578</td>\n",
       "      <td>0.803286</td>\n",
       "      <td>0.586578</td>\n",
       "      <td>0.3556</td>\n",
       "      <td>0.442777</td>\n",
       "      <td>0.802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>rfe_feat_dataset</td>\n",
       "      <td>Nearest Neighbors</td>\n",
       "      <td>0.595596</td>\n",
       "      <td>0.349408</td>\n",
       "      <td>0.440434</td>\n",
       "      <td>0.803619</td>\n",
       "      <td>0.602967</td>\n",
       "      <td>0.347062</td>\n",
       "      <td>0.440548</td>\n",
       "      <td>0.805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>corr_feat_dataset</td>\n",
       "      <td>Nearest Neighbors</td>\n",
       "      <td>0.586531</td>\n",
       "      <td>0.33563</td>\n",
       "      <td>0.426948</td>\n",
       "      <td>0.800714</td>\n",
       "      <td>0.565397</td>\n",
       "      <td>0.343044</td>\n",
       "      <td>0.427008</td>\n",
       "      <td>0.796333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>lasso_feat_dataset</td>\n",
       "      <td>Nearest Neighbors</td>\n",
       "      <td>0.524064</td>\n",
       "      <td>0.358665</td>\n",
       "      <td>0.425869</td>\n",
       "      <td>0.786095</td>\n",
       "      <td>0.526122</td>\n",
       "      <td>0.359116</td>\n",
       "      <td>0.426866</td>\n",
       "      <td>0.786667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>all_feat_dataset</td>\n",
       "      <td>Nearest Neighbors</td>\n",
       "      <td>0.588826</td>\n",
       "      <td>0.315393</td>\n",
       "      <td>0.410767</td>\n",
       "      <td>0.799857</td>\n",
       "      <td>0.610169</td>\n",
       "      <td>0.307383</td>\n",
       "      <td>0.408818</td>\n",
       "      <td>0.803333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>rfe_feat_dataset</td>\n",
       "      <td>Quadratic DA</td>\n",
       "      <td>0.251435</td>\n",
       "      <td>0.895587</td>\n",
       "      <td>0.392638</td>\n",
       "      <td>0.387143</td>\n",
       "      <td>0.265319</td>\n",
       "      <td>0.850326</td>\n",
       "      <td>0.404443</td>\n",
       "      <td>0.446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>rfe_feat_dataset</td>\n",
       "      <td>Gaussian NB</td>\n",
       "      <td>0.253456</td>\n",
       "      <td>0.892142</td>\n",
       "      <td>0.394761</td>\n",
       "      <td>0.394905</td>\n",
       "      <td>0.25218</td>\n",
       "      <td>0.885987</td>\n",
       "      <td>0.392611</td>\n",
       "      <td>0.393556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>all_feat_dataset</td>\n",
       "      <td>Quadratic DA</td>\n",
       "      <td>0.241584</td>\n",
       "      <td>0.94704</td>\n",
       "      <td>0.384965</td>\n",
       "      <td>0.330667</td>\n",
       "      <td>0.233574</td>\n",
       "      <td>0.958815</td>\n",
       "      <td>0.37564</td>\n",
       "      <td>0.294889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>lasso_feat_dataset</td>\n",
       "      <td>Quadratic DA</td>\n",
       "      <td>0.24796</td>\n",
       "      <td>0.90915</td>\n",
       "      <td>0.389648</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.229093</td>\n",
       "      <td>0.982421</td>\n",
       "      <td>0.371545</td>\n",
       "      <td>0.264778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>all_feat_dataset</td>\n",
       "      <td>Gaussian NB</td>\n",
       "      <td>0.231921</td>\n",
       "      <td>0.970721</td>\n",
       "      <td>0.374393</td>\n",
       "      <td>0.282429</td>\n",
       "      <td>0.227927</td>\n",
       "      <td>0.978905</td>\n",
       "      <td>0.369759</td>\n",
       "      <td>0.261778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>lasso_feat_dataset</td>\n",
       "      <td>Gaussian NB</td>\n",
       "      <td>0.225721</td>\n",
       "      <td>0.990743</td>\n",
       "      <td>0.367675</td>\n",
       "      <td>0.246238</td>\n",
       "      <td>0.225884</td>\n",
       "      <td>0.991462</td>\n",
       "      <td>0.36794</td>\n",
       "      <td>0.246444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          feats_subset                 algo train-precision train-recall  \\\n",
       "43    all_feat_dataset        Random Forest        0.485351     0.599139   \n",
       "23   tree_feat_dataset        Random Forest          0.4913     0.589666   \n",
       "3    corr_feat_dataset        Random Forest         0.50257     0.568353   \n",
       "2    corr_feat_dataset        Decision Tree        0.500469     0.573735   \n",
       "13  lasso_feat_dataset        Random Forest        0.498963     0.569645   \n",
       "33    rfe_feat_dataset        Random Forest        0.459343      0.59591   \n",
       "12  lasso_feat_dataset        Decision Tree        0.498018     0.567922   \n",
       "22   tree_feat_dataset        Decision Tree        0.458018     0.586006   \n",
       "42    all_feat_dataset        Decision Tree        0.457835     0.585576   \n",
       "8    corr_feat_dataset              XGBoost        0.468478     0.567922   \n",
       "26   tree_feat_dataset          Gaussian NB        0.361323     0.681808   \n",
       "28   tree_feat_dataset              XGBoost        0.462034     0.555436   \n",
       "1    corr_feat_dataset  Logistic Regression        0.355861      0.65296   \n",
       "48    all_feat_dataset              XGBoost        0.468183     0.567061   \n",
       "27   tree_feat_dataset         Quadratic DA        0.320565     0.753068   \n",
       "32    rfe_feat_dataset        Decision Tree        0.411739     0.601076   \n",
       "11  lasso_feat_dataset  Logistic Regression        0.397337     0.610334   \n",
       "18  lasso_feat_dataset              XGBoost        0.447779     0.575027   \n",
       "6    corr_feat_dataset          Gaussian NB        0.572261     0.401507   \n",
       "21   tree_feat_dataset  Logistic Regression        0.373769     0.645425   \n",
       "41    all_feat_dataset  Logistic Regression        0.372537     0.651238   \n",
       "38    rfe_feat_dataset              XGBoost         0.45879      0.55845   \n",
       "14  lasso_feat_dataset       MLP Neural Net        0.684605     0.334123   \n",
       "31    rfe_feat_dataset  Logistic Regression        0.367093     0.648439   \n",
       "7    corr_feat_dataset         Quadratic DA        0.530036     0.440689   \n",
       "9    corr_feat_dataset             LightGBM        0.668672     0.358881   \n",
       "4    corr_feat_dataset       MLP Neural Net        0.679117     0.344456   \n",
       "19  lasso_feat_dataset             LightGBM        0.672617     0.356943   \n",
       "5    corr_feat_dataset             AdaBoost        0.684423     0.329171   \n",
       "24   tree_feat_dataset       MLP Neural Net         0.68174     0.340797   \n",
       "29   tree_feat_dataset             LightGBM        0.663133     0.363617   \n",
       "35    rfe_feat_dataset             AdaBoost         0.69145     0.320344   \n",
       "49    all_feat_dataset             LightGBM        0.668914     0.363186   \n",
       "39    rfe_feat_dataset             LightGBM        0.670727     0.355651   \n",
       "44    all_feat_dataset       MLP Neural Net        0.670628     0.340151   \n",
       "15  lasso_feat_dataset             AdaBoost        0.690625     0.333046   \n",
       "45    all_feat_dataset             AdaBoost         0.68678      0.32099   \n",
       "34    rfe_feat_dataset       MLP Neural Net        0.697381      0.30958   \n",
       "25   tree_feat_dataset             AdaBoost        0.684524     0.321851   \n",
       "20   tree_feat_dataset    Nearest Neighbors        0.591589     0.357374   \n",
       "30    rfe_feat_dataset    Nearest Neighbors        0.595596     0.349408   \n",
       "0    corr_feat_dataset    Nearest Neighbors        0.586531      0.33563   \n",
       "10  lasso_feat_dataset    Nearest Neighbors        0.524064     0.358665   \n",
       "40    all_feat_dataset    Nearest Neighbors        0.588826     0.315393   \n",
       "37    rfe_feat_dataset         Quadratic DA        0.251435     0.895587   \n",
       "36    rfe_feat_dataset          Gaussian NB        0.253456     0.892142   \n",
       "47    all_feat_dataset         Quadratic DA        0.241584      0.94704   \n",
       "17  lasso_feat_dataset         Quadratic DA         0.24796      0.90915   \n",
       "46    all_feat_dataset          Gaussian NB        0.231921     0.970721   \n",
       "16  lasso_feat_dataset          Gaussian NB        0.225721     0.990743   \n",
       "\n",
       "    train-f1 train-acc val-precision val-recall    val-f1   val-acc  \n",
       "43  0.536275   0.77081      0.514706    0.59769  0.553102  0.786333  \n",
       "23  0.536008   0.77419      0.512364   0.593169  0.549814  0.785111  \n",
       "3   0.533441  0.780095      0.523481    0.57107  0.546241  0.790111  \n",
       "2   0.534604  0.779048      0.501485   0.593672  0.543698  0.779556  \n",
       "13  0.531966  0.778286      0.520441   0.569061  0.543666  0.788667  \n",
       "33  0.518789  0.755476      0.492687   0.592165  0.537865  0.774889  \n",
       "12  0.530678   0.77781      0.492575   0.583124  0.534039  0.774889  \n",
       "22  0.514167  0.755048      0.459376   0.599196  0.520052  0.755333  \n",
       "42  0.513886  0.754952      0.459168   0.598694   0.51973  0.755222  \n",
       "8   0.513429  0.761905      0.460025   0.563536  0.506546  0.757111  \n",
       "26  0.472334  0.663048      0.414977   0.637368  0.502674     0.721  \n",
       "28  0.504448  0.758619      0.494163   0.510296    0.5021  0.776111  \n",
       "1   0.460662   0.66181      0.417991   0.620794  0.499596  0.724889  \n",
       "48    0.5129  0.761762      0.490998   0.506781  0.498764  0.774667  \n",
       "27  0.449701  0.592333      0.395578   0.664992  0.496066  0.701111  \n",
       "32   0.48871   0.72181      0.405848   0.634355  0.495003  0.713667  \n",
       "11  0.481324  0.709048      0.406645   0.614766  0.489502  0.716333  \n",
       "18  0.503487  0.749143      0.445873    0.53993  0.488414  0.749778  \n",
       "6   0.471913  0.801238      0.573379   0.421899  0.486111  0.802667  \n",
       "21  0.473393  0.682381      0.395768   0.629332  0.485941  0.705444  \n",
       "41  0.473952  0.680238       0.38911   0.638875   0.48365  0.698222  \n",
       "38  0.503738  0.756619      0.474477   0.490206  0.482213  0.767111  \n",
       "14  0.449074  0.818667      0.687324   0.367654  0.479058  0.823111  \n",
       "31  0.468794  0.674952      0.390228    0.61778   0.47832  0.701889  \n",
       "7   0.481251  0.789857      0.549175    0.41788  0.474615  0.795333  \n",
       "9   0.467078  0.818857      0.646649   0.373179  0.473248  0.816222  \n",
       "4   0.457078     0.819      0.672862   0.363636  0.472123  0.820111  \n",
       "19  0.466385  0.819333      0.647318   0.369663  0.470588     0.816  \n",
       "5   0.444541  0.818048      0.680344   0.358112  0.469233  0.820778  \n",
       "24  0.454428     0.819      0.678811     0.3556  0.466711  0.820222  \n",
       "29  0.469689  0.818381      0.636284   0.364641  0.463602  0.813333  \n",
       "35   0.43784  0.818048      0.700203   0.346057  0.463193  0.822556  \n",
       "49  0.470769  0.819381      0.640071   0.362632  0.462969  0.813889  \n",
       "39  0.464828  0.818857       0.64808   0.356102  0.459643  0.814778  \n",
       "44  0.451364  0.817095      0.688064    0.34455   0.45917  0.820444  \n",
       "15  0.449383  0.819476      0.687375    0.34455  0.459016  0.820333  \n",
       "45    0.4375  0.817429      0.698869   0.341537  0.458839  0.821778  \n",
       "34  0.428806  0.817571      0.705696   0.336012  0.455257  0.822111  \n",
       "25  0.437839   0.81719       0.69413   0.338523  0.455098  0.820667  \n",
       "20  0.445578  0.803286      0.586578     0.3556  0.442777     0.802  \n",
       "30  0.440434  0.803619      0.602967   0.347062  0.440548     0.805  \n",
       "0   0.426948  0.800714      0.565397   0.343044  0.427008  0.796333  \n",
       "10  0.425869  0.786095      0.526122   0.359116  0.426866  0.786667  \n",
       "40  0.410767  0.799857      0.610169   0.307383  0.408818  0.803333  \n",
       "37  0.392638  0.387143      0.265319   0.850326  0.404443     0.446  \n",
       "36  0.394761  0.394905       0.25218   0.885987  0.392611  0.393556  \n",
       "47  0.384965  0.330667      0.233574   0.958815   0.37564  0.294889  \n",
       "17  0.389648      0.37      0.229093   0.982421  0.371545  0.264778  \n",
       "46  0.374393  0.282429      0.227927   0.978905  0.369759  0.261778  \n",
       "16  0.367675  0.246238      0.225884   0.991462   0.36794  0.246444  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelling_df.sort_values(['val-f1','val-precision','val-recall'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "886cc3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling_df.sort_values(['val-f1','val-precision','val-recall'], ascending=False).to_csv(\"../outputs/model_benchmarks.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a027d9df",
   "metadata": {},
   "source": [
    "### Fine-tuning best performers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86737a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 48 candidates, totalling 720 fits\n",
      "Best RF Params: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 400}\n",
      "Best RF ROC-AUC: 0.7810074217845973\n",
      "Fitting 15 folds for each of 72 candidates, totalling 1080 fits\n",
      "Best XGB Params: {'colsample_bytree': 0.7, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 400, 'subsample': 0.7}\n",
      "Best XGB ROC-AUC: 0.7845218191958593\n",
      "Fitting 15 folds for each of 216 candidates, totalling 3240 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# ------------------------------\n",
    "# Prepare Data\n",
    "# ------------------------------\n",
    "X_tuned = StandardScaler().fit_transform(X)   # Optional for trees, but consistent\n",
    "y_tuned = y\n",
    "\n",
    "# 5-fold CV repeated 3 times\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=29)\n",
    "\n",
    "# ==========================================================\n",
    "# 1. RANDOM FOREST TUNING\n",
    "# ==========================================================\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [200, 400],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=29, n_jobs=-1)\n",
    "\n",
    "rf_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=rf_param_grid,\n",
    "    scoring='roc_auc',\n",
    "    cv=cv,\n",
    "    verbose=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_search.fit(X_tuned, y_tuned)\n",
    "print(\"Best RF Params:\", rf_search.best_params_)\n",
    "print(\"Best RF ROC-AUC:\", rf_search.best_score_)\n",
    "\n",
    "# ==========================================================\n",
    "# 2. XGBOOST TUNING\n",
    "# ==========================================================\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [200, 400],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.7, 1.0],\n",
    "    'colsample_bytree': [0.7, 1.0]\n",
    "}\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    random_state=29,\n",
    "    eval_metric='logloss',\n",
    "    tree_method='hist',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_search = GridSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_grid=xgb_param_grid,\n",
    "    scoring='roc_auc',\n",
    "    cv=cv,\n",
    "    verbose=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_search.fit(X_tuned, y_tuned)\n",
    "print(\"Best XGB Params:\", xgb_search.best_params_)\n",
    "print(\"Best XGB ROC-AUC:\", xgb_search.best_score_)\n",
    "\n",
    "# ==========================================================\n",
    "# 3. LIGHTGBM TUNING\n",
    "# ==========================================================\n",
    "lgb_param_grid = {\n",
    "    'n_estimators': [200, 400],\n",
    "    'num_leaves': [31, 63, 127],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [-1, 10, 20],\n",
    "    'subsample': [0.7, 1.0],\n",
    "    'colsample_bytree': [0.7, 1.0]\n",
    "}\n",
    "\n",
    "lgb = LGBMClassifier(\n",
    "    random_state=29,\n",
    "    boosting_type='gbdt',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lgb_search = GridSearchCV(\n",
    "    estimator=lgb,\n",
    "    param_grid=lgb_param_grid,\n",
    "    scoring='roc_auc',\n",
    "    cv=cv,\n",
    "    verbose=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lgb_search.fit(X_tuned, y_tuned)\n",
    "print(\"Best LGBM Params:\", lgb_search.best_params_)\n",
    "print(\"Best LGBM ROC-AUC:\", lgb_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be31d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import RandomizedSearchCV, RepeatedStratifiedKFold\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "# from lightgbm import LGBMClassifier\n",
    "# from scipy.stats import randint, uniform\n",
    "\n",
    "# # ------------------------------\n",
    "# # Prepare Data\n",
    "# # ------------------------------\n",
    "# X_tuned = StandardScaler().fit_transform(X)   # Optional for trees\n",
    "# y_tuned = y\n",
    "\n",
    "# # CV setup (5 folds × 3 repeats)\n",
    "# cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=29)\n",
    "\n",
    "# # ==========================================================\n",
    "# # 1. RANDOM FOREST (FAST RANDOM SEARCH)\n",
    "# # ==========================================================\n",
    "# rf_param_dist = {\n",
    "#     'n_estimators': randint(200, 800),\n",
    "#     'max_depth': randint(4, 30),\n",
    "#     'min_samples_split': randint(2, 10),\n",
    "#     'min_samples_leaf': randint(1, 5),\n",
    "#     'max_features': ['sqrt', 'log2', None]\n",
    "# }\n",
    "\n",
    "# rf = RandomForestClassifier(random_state=29, n_jobs=-1)\n",
    "\n",
    "# rf_search = RandomizedSearchCV(\n",
    "#     estimator=rf,\n",
    "#     param_distributions=rf_param_dist,\n",
    "#     n_iter=30,                   # Faster\n",
    "#     scoring='roc_auc',\n",
    "#     cv=cv,\n",
    "#     verbose=3,\n",
    "#     random_state=29,\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "\n",
    "# rf_search.fit(X_tuned, y_tuned)\n",
    "# print(\"Best RF Params:\", rf_search.best_params_)\n",
    "# print(\"Best RF ROC-AUC:\", rf_search.best_score_)\n",
    "\n",
    "\n",
    "# # ==========================================================\n",
    "# # 2. XGBOOST RANDOM SEARCH\n",
    "# # ==========================================================\n",
    "# xgb_param_dist = {\n",
    "#     'n_estimators': randint(200, 700),\n",
    "#     'max_depth': randint(3, 10),\n",
    "#     'learning_rate': uniform(0.01, 0.2),\n",
    "#     'subsample': uniform(0.6, 0.4),\n",
    "#     'colsample_bytree': uniform(0.6, 0.4),\n",
    "#     'gamma': uniform(0, 5),\n",
    "# }\n",
    "\n",
    "# xgb = XGBClassifier(\n",
    "#     random_state=29,\n",
    "#     eval_metric='logloss',\n",
    "#     tree_method='hist',\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "\n",
    "# xgb_search = RandomizedSearchCV(\n",
    "#     estimator=xgb,\n",
    "#     param_distributions=xgb_param_dist,\n",
    "#     n_iter=40,                  # More iterations for XGB\n",
    "#     scoring='roc_auc',\n",
    "#     cv=cv,\n",
    "#     verbose=3,\n",
    "#     random_state=29,\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "\n",
    "# xgb_search.fit(X_tuned, y_tuned)\n",
    "# print(\"Best XGB Params:\", xgb_search.best_params_)\n",
    "# print(\"Best XGB ROC-AUC:\", xgb_search.best_score_)\n",
    "\n",
    "\n",
    "# # ==========================================================\n",
    "# # 3. LIGHTGBM RANDOM SEARCH\n",
    "# # ==========================================================\n",
    "# lgb_param_dist = {\n",
    "#     'n_estimators': randint(200, 700),\n",
    "#     'num_leaves': randint(31, 255),\n",
    "#     'learning_rate': uniform(0.01, 0.2),\n",
    "#     'max_depth': randint(-1, 20),\n",
    "#     'subsample': uniform(0.6, 0.4),\n",
    "#     'colsample_bytree': uniform(0.6, 0.4)\n",
    "# }\n",
    "\n",
    "# lgb = LGBMClassifier(\n",
    "#     random_state=29,\n",
    "#     boosting_type='gbdt',\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "\n",
    "# lgb_search = RandomizedSearchCV(\n",
    "#     estimator=lgb,\n",
    "#     param_distributions=lgb_param_dist,\n",
    "#     n_iter=40,                 # More iterations → better\n",
    "#     scoring='roc_auc',\n",
    "#     cv=cv,\n",
    "#     verbose=3,\n",
    "#     random_state=29,\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "\n",
    "# lgb_search.fit(X_tuned, y_tuned)\n",
    "# print(\"Best LGBM Params:\", lgb_search.best_params_)\n",
    "# print(\"Best LGBM ROC-AUC:\", lgb_search.best_score_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adcb-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
